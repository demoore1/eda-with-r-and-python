{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842e6fe6-650d-4c9a-b82f-1e28f27b7ad9",
   "metadata": {},
   "source": [
    "```{thebe-button}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ad23b-1981-4d12-9705-0a09916a0fd3",
   "metadata": {},
   "source": [
    "# 02: Data Cleaning\n",
    "\n",
    "**Objective:** Identify and handle missing or invalid values, detect outliers, and standardize data for our recidivism dataset.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "<table width=\"100%\">\n",
    "  <tr>\n",
    "    <td style=\"vertical-align: top; text-align: left; width: 60%; padding-right: 20px;\">\n",
    "      <ol style=\"font-size: 20px; line-height: 1.4;\">\n",
    "        <li>Identify &amp; quantify missing data</li>\n",
    "        <li>Handle missing values (imputation, removal, flagging)</li>\n",
    "        <li>Validate &amp; correct data types</li>\n",
    "        <li>Detect &amp; treat outliers</li>\n",
    "        <li>Standardize &amp; normalize</li>\n",
    "        <li>Document transformations</li>\n",
    "      </ol>\n",
    "    </td>\n",
    "    <td style=\"vertical-align: top; text-align: left; width: 40%;\">\n",
    "      <!-- relative path to cleaning.png -->\n",
    "      <img src=\"../slides/cleaning.png\" alt=\"Data Cleaning Image\" width=\"1000\" />\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "<audio controls src=\"../audio/Cleaning.m4a\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038bfc3-de7e-4ac8-9bfd-3eeb5d0a804a",
   "metadata": {},
   "source": [
    "\n",
    "Now that we’ve laid out our data‑cleaning roadmap, let’s put it into practice with a real dataset. For the rest of this lesson, we’ll work with the **COMPAS recidivism data** (`compas‑scores‑raw.csv`), which contains demographic and risk‑assessment scores for individuals screened by the COMPAS tool. \n",
    "\n",
    "We’ll start by running through each cleaning step in **Python**, then you are encouraged to repeat the same process in **R** so you can see both workflows side by side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0793dae-61a0-420c-856e-58e9bc484752",
   "metadata": {},
   "source": [
    "\n",
    "Now that we’ve laid out our data‑cleaning roadmap, let’s put it into practice with a real dataset. For the rest of this lesson, we’ll work with the **COMPAS recidivism data** (`compas‑scores‑raw.csv`), which contains demographic and risk‑assessment scores for individuals screened by the COMPAS tool. \n",
    "\n",
    "We’ll start by running through each cleaning step in **Python**, then you are encouraged to repeat the same process in **R** so you can see both workflows side by side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a89e64-3f04-492d-a262-0b084cb257f7",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "Since this is your first time, you’ll need to download all of the little helper programs (called “packages”) we’ll use—just run one simple command to get them all at once:\n",
    "\n",
    "```bash\n",
    "!pip install -r requirements_Python.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e37b8-c057-4f94-92da-b657ce1b597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this block your first time doing this training.\n",
    "\n",
    "!pip install -r ..\\requirements_Python.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55255904-4cf6-40f7-9c67-4c39aa20a16b",
   "metadata": {},
   "source": [
    "#### Loading the Pandas Library\n",
    "\n",
    "Before we can work with our data in Python, we need to load the **Pandas** library. The line below does two things:\n",
    "\n",
    "1. **Imports** the Pandas module so its functions and classes become available.  \n",
    "2. **Aliases** it as `pd` so we can reference it concisely in our code.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf710d-0997-4320-89a7-1f792fdbfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5011f82-3e85-4121-bb24-738bbc06b09f",
   "metadata": {},
   "source": [
    "### 2.1 Identify & Quantify Missing Data\n",
    "\n",
    "First, load the data and get a sense of where values are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a574c5c-4276-48df-a27b-b41469f4dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/compas_scores_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cc82c-e2bd-4c5a-8b81-a49a1c75e69c",
   "metadata": {},
   "source": [
    "Finally, we call .head() on the DataFrame to display the first five rows. This gives us a quick peek at the structure and contents of our dataset, including column names and sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b49fc-0bfa-457f-bab7-dc0da5595a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942fdb02-9d2f-4ecc-8906-9b595738b146",
   "metadata": {},
   "source": [
    "> **What to look for in the output:**\n",
    "> - **Column names** such as `Person_ID`, `Case_ID`, `LastName`, `FirstName`, `MiddleName`, `Sex_Code_Text`, `Ethnic_Code_Text`, `RawScore`, `DecileScore`, `ScoreText`, `Age`  \n",
    "> - **Data types** (numeric vs. categorical) and any surprising or missing values  \n",
    "> - **Structure** of the dataset—how it's organized before we begin cleaning and analysis  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510fc8c-43d8-4723-b1b3-f2fca89eda92",
   "metadata": {},
   "source": [
    "Now that we’ve successfully loaded the COMPAS dataset into our `df` object, it’s time to get a quick overview of its structure. In the next step, we’ll check how big the table is and see where any missing values might be hiding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649938c-3bdb-4a87-bc70-e89f1d0d5c4e",
   "metadata": {},
   "source": [
    "#### Checking Dataset Size and Missing Data\n",
    "\n",
    "In this step, we first look at the overall size of our dataset and then count any empty or missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa34fe0-170a-4271-a4a2-e056a4ff0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468eb131-7f1b-43cb-a132-7651db4ebb18",
   "metadata": {},
   "source": [
    " **What does the output mean?**\n",
    "\n",
    " - **Dataset shape**  \n",
    "   This tells us how many rows (records) and columns (fields) we have.  \n",
    "   For example, `(24272, 24)` means 24,072 rows and 24 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d22cf-ddc7-41f1-a889-98895d1e485a",
   "metadata": {},
   "source": [
    "### 2.2 Handle Missing Values\n",
    "\n",
    "#### Counting Missing Values\n",
    "\n",
    "First, let’s see which columns have missing data and how many blanks each contains:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c38308-8320-4cac-b416-c7944dc88277",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isna().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c014f-e3be-4013-b172-846f72d5f9cd",
   "metadata": {},
   "source": [
    "#### Options for Handling Missing Values\n",
    "- **Removal:** drop rows or columns with too many nulls  \n",
    "- **Imputation:** fill with mean/median or a constant  \n",
    "\n",
    "##### MiddleName: 17,942 missing values  \n",
    "  Most records don’t include a middle name—since this field isn’t critical to our analysis, we’ll drop the entire column.\n",
    "\n",
    "##### ScoreText: 45 missing values  \n",
    "  A small number of records lack the textual risk label. We’ll remove any rows where `ScoreText` is missing, since we need that label for downstream analyses.\n",
    "\n",
    "##### Age: 56 missing values\n",
    "  A handful of entries are missing age information. Since age is important for our analysis, we’ll impute these missing ages with the median age value.\n",
    "\n",
    "By printing only `missing[missing > 0]`, we focus on the columns that actually have missing entries, allowing us to target our cleaning efforts precisely.  \n",
    "\n",
    "Below we’ll:\n",
    "1. Drop columns with >50% missing  \n",
    "2. Impute `Age` with the median  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6dfc9-469d-4194-94a2-2fb7dc52fb16",
   "metadata": {},
   "source": [
    "##### Step 1: Drop Mostly Empty Columns\n",
    "\n",
    "Our dataset has a “MiddleName\" column that’s almost entirely blank. To avoid clutter, we remove any column where more than half of its values are missing. This will automatically drop “MiddleName” and any other mostly empty fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b0235-1a87-4a1f-a943-0a16823a39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = len(df) * 0.5\n",
    "df = df.dropna(axis=1, thresh=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13cd5c-24b2-485e-b5e7-24a98654bc90",
   "metadata": {},
   "source": [
    "##### Step 2: Remove Rows with Blank `ScoreText` and Fill Missing `Age`\n",
    "\n",
    "First, we drop any rows where `ScoreText` is blank, since those entries don’t tell us whether someone was classified as “Low,” “Medium,” or “High” risk. After that, we flag rows with missing `Age`, compute the median age, and fill those gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a179a6-9449-41ab-aebe-abcb08f0fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop rows where ScoreText is blank\n",
    "df = df[df[\"ScoreText\"].str.strip() != \"\"]\n",
    "\n",
    "# 2. Flag rows where Age was missing\n",
    "df[\"age_missing\"] = df[\"Age\"].isna()\n",
    "\n",
    "# 3. Compute median Age and fill missing ages\n",
    "median_age = df[\"Age\"].median()\n",
    "df[\"Age\"] = df[\"Age\"].fillna(median_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051425e-4eda-4564-8147-5cc5f3b160fc",
   "metadata": {},
   "source": [
    "##### Now we have:\n",
    "- Removed all rows lacking a valid risk category in `ScoreText`.  \n",
    "- Marked originally missing `Age` values in a new `age_missing` column.  \n",
    "- Filled those missing ages with the dataset’s median age.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a676468-d76f-4736-a5d3-8150986efdcb",
   "metadata": {},
   "source": [
    "### 2.3 Validate & Correct Data Types\n",
    "\n",
    "Before we go further, let’s make sure each column uses the right data type:\n",
    "\n",
    "- Numbers as integers or floats  \n",
    "- Dates as datetime objects  \n",
    "- Text fields we’ll analyze categorically as “category”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9248f9-6f76-4a57-b36b-7867c6039cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Score and Age to integers\n",
    "df[\"RawScore\"]    = df[\"RawScore\"].astype(int)\n",
    "df[\"DecileScore\"] = df[\"DecileScore\"].astype(int)\n",
    "df[\"Age\"]         = df[\"Age\"].astype(int)\n",
    "\n",
    "\n",
    "# Convert text fields to categorical\n",
    "for col in [\n",
    "    \"ScoreText\", \"Sex_Code_Text\", \"Ethnic_Code_Text\",\n",
    "    \"Language\", \"MaritalStatus\", \"RecSupervisionLevelText\"\n",
    "]:\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Verify types without any warnings\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab059637-4a4c-44ce-9d35-1c3a81e733cb",
   "metadata": {},
   "source": [
    "> **What we’ve done:**  \n",
    "> - Forced `RawScore`, `DecileScore`, and `Age` into integer form  \n",
    "> - Marked risk categories and demographic fields as categorical  \n",
    "> - Verified the changes by inspecting `df.dtypes`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90459761-c670-4253-b737-c765a891c30f",
   "metadata": {},
   "source": [
    "### Step 2.4: Find and Remove Extreme Values (Outliers) in `RawScore`\n",
    "\n",
    "Even smart data can hide a few extreme values—called **outliers**—that skew our insights. We’ll clean those out by:\n",
    "\n",
    "1. **Sorting** all `RawScore` values and finding the 25th percentile (Q1) and 75th percentile (Q3).  \n",
    "2. Calculating the **interquartile range (IQR)** = Q3 − Q1, which covers the middle 50% of the data.  \n",
    "3. Defining an **acceptable range** as   [Q1 − 3×IQR, Q3 + 3×IQR]\n",
    "4. **Keeping** only rows with `RawScore` inside that range and dropping the rest.\n",
    "\n",
    "This preserves most of the data while removing the very highest or lowest scores that could mislead our analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f83002-753a-4212-91ee-eb720f8fff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Q1 (25th percentile) and Q3 (75th percentile) for RawScore\n",
    "Q1 = df[\"RawScore\"].quantile(0.25)\n",
    "Q3 = df[\"RawScore\"].quantile(0.75)\n",
    "\n",
    "# 2. Compute the IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Define acceptable lower and upper bounds\n",
    "lower_bound = Q1 - 3 * IQR\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "# 4. Filter the dataframe to keep only non-outliers\n",
    "df_clean = df[(df[\"RawScore\"] >= lower_bound) & (df[\"RawScore\"] <= upper_bound)]\n",
    "\n",
    "# 5. Report how many rows we kept versus removed\n",
    "print(f\"Rows before cleaning: {len(df)}\")\n",
    "print(f\"Rows after removing outliers: {len(df_clean)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fd849-13f2-492f-9086-ccd8fb65171a",
   "metadata": {},
   "source": [
    "> **In plain terms:**  \n",
    "> - We measured how wide the middle 50% of scores is.  \n",
    "> - We dropped any scores more than three times that range away from the center.  \n",
    "> - Now `df_clean` has most of our original rows minus the extreme cases that could distort averages or trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9305e02-8cce-416a-9faf-4598ef547084",
   "metadata": {},
   "source": [
    "### Step 2.5: Standardize & Normalize Key Variables\n",
    "\n",
    "Some analysis methods work best when numbers share a common scale. We’ll use two popular Python libraries:\n",
    "\n",
    "- **SciPy** (`scipy`): a scientific computing library that includes statistical functions, like calculating z‑scores.  \n",
    "- **scikit‑learn** (`sklearn`): a machine‑learning library that provides tools for data preprocessing, including min‑max scaling.\n",
    "\n",
    "Here’s what we’ll do:\n",
    "\n",
    "1. **Z‑score Standardization** on `Age`  \n",
    "   - Subtract the average age, then divide by the age’s standard deviation  \n",
    "   - Result is a new column `age_z` where most values fall between –3 and +3  \n",
    "\n",
    "2. **Min‑Max Scaling** on `RawScore`  \n",
    "   - Rescales `RawScore` to lie between 0 (lowest score) and 1 (highest score)  \n",
    "   - Result is a new column `rawscore_scaled` that preserves relative differences  \n",
    "\n",
    "We’ll apply both to our cleaned data in `df_clean`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484e3c0-d6a4-4bab-9ca7-9248ff4722c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Z‑score standardization on Age\n",
    "df_clean.loc[:, \"age_z\"] = stats.zscore(df_clean[\"Age\"])\n",
    "\n",
    "# 2. Min‑max scaling on RawScore\n",
    "scaler = MinMaxScaler()\n",
    "df_clean.loc[:, \"rawscore_scaled\"] = scaler.fit_transform(df_clean[[\"RawScore\"]])\n",
    "\n",
    "# Show the new columns alongside the originals\n",
    "df_clean[[\"Age\", \"age_z\", \"RawScore\", \"rawscore_scaled\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b2583-9551-4f17-b226-dd0baa874704",
   "metadata": {},
   "source": [
    "## Interpreting Our Transformed Data\n",
    "\n",
    "\n",
    "\n",
    "| Age | age_z | RawScore | rawscore_scaled |\n",
    "| --- | ----- | -------- | --------------- |\n",
    "| 20  | -1.057784 | 15       | 0.3333      |\n",
    "| 28  | -0.387237 | 19       | 0.4222      |\n",
    "| 18  | -1.225421 | 0        | 0.0000      |\n",
    "| 18  | -1.225421 | 13       | 0.2888      |\n",
    "\n",
    "### What each column is\n",
    "\n",
    "- **Age**  \n",
    "  The actual age in years\n",
    "\n",
    "- **age_z**  \n",
    "  A “z‑score” shows how far each age is from the average age, measured in standard units  \n",
    "  - 0 means exactly average  \n",
    "  - Negative means below average  \n",
    "  - Positive means above average  \n",
    "  - −-1.05778 means about -1.05778 units younger than average\n",
    "\n",
    "- **RawScore**  \n",
    "  The original score before any changes (for example a test result or rating)\n",
    "\n",
    "- **rawscore_scaled**  \n",
    "  The RawScore rescaled to lie between 0 and 1  \n",
    "  - 0 is the lowest RawScore in our group  \n",
    "  - 1 is the highest RawScore in our group  \n",
    "  - 0.03 means very close to the lowest  \n",
    "  - 0.29 means 29 percent of the way from lowest to highest\n",
    "\n",
    "### Why we do this\n",
    "\n",
    "1. **Fair comparison**  \n",
    "   When columns use very different units or ranges our analysis can get biased. Standardizing and scaling puts all numbers on the same footing.\n",
    "\n",
    "2. **Better results**  \n",
    "   Many data tools and machine learning methods work best when inputs live on the same scale.\n",
    "\n",
    "3. **Clear interpretation**  \n",
    "   - Z‑scores tell us how far a value is from the average in comparable units  \n",
    "   - Scaled scores between 0 and 1 make it easy to see relative position without worrying about original units\n",
    "\n",
    "---\n",
    "\n",
    "In plain terms, we’ve “translated” every number so they all speak the same language. That helps our next analysis steps work properly and gives you a fair way to compare apples to apples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cb9c7-04e7-45fd-b2b9-004b3b8a3bea",
   "metadata": {},
   "source": [
    "### Step 2.6: Record Your Cleaning Steps\n",
    "\n",
    "Finally, it’s best practice to keep a log of every transformation. Below we build a simple dictionary summarizing what we did:\n",
    "\n",
    "- Which columns we dropped because they were mostly empty  \n",
    "- The median age we used for imputation  \n",
    "- The bounds we used to remove outliers in `RawScore`  \n",
    "- Which columns we standardized and normalized  \n",
    "\n",
    "At the end, we print this log in a clear, readable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5701e29-38bf-433d-b76c-84d9a9c48969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of (Step, Details)\n",
    "log_items = [\n",
    "    (\"Dropped Columns\", list(missing[missing > threshold].index)),\n",
    "    (\"Imputed Age\", median_age),\n",
    "    (\"Outlier Bounds (RawScore)\", (lower_bound, upper_bound)),\n",
    "    (\"Standardized\", [\"age_z\"]),\n",
    "    (\"Normalized\", [\"rawscore_scaled\"])\n",
    "]\n",
    "\n",
    "# Turn it into a DataFrame and display\n",
    "log_df = pd.DataFrame(log_items, columns=[\"Step\", \"Details\"])\n",
    "display(log_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a29431-5768-43eb-ac84-f569451d0c8f",
   "metadata": {},
   "source": [
    " **How to read this table:**  \n",
    " - **Step:** what we did  \n",
    " - **Details:** the exact values or columns affected by that step  \n",
    "\n",
    "\n",
    "> **Why this matters:**  \n",
    "> A clear transformation log makes your work reproducible and lets others (or future you) understand exactly how the data was prepared before any analysis or modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb9581-270a-4f0d-8b8f-15f0071ef959",
   "metadata": {},
   "source": [
    "## Next Steps: Continue in Python or Switch to R\n",
    "\n",
    "Our data is now clean and properly formatted. You have two options:\n",
    "\n",
    "1. **Try the R version** of this cleaning workflow by opening `02_data_cleaning_R.ipynb`.  \n",
    "2. **Move on to summary statistics in Python** by opening `03_summary_statistics_Python.ipynb`.  \n",
    "\n",
    "Choose the notebook that matches your preferred language, and let’s continue exploring our recidivism dataset!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deef58e-f553-4af7-af4e-f869c23c8368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
